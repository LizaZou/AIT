{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIemhj05OZ3k","executionInfo":{"status":"ok","timestamp":1662002008283,"user_tz":-420,"elapsed":29754,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"27d083ae-f95a-46b2-9beb-9aba7b14fdb2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["0-IMPORTING PYTHON PACKAGES AND LIBRARIES"],"metadata":{"id":"ts60gs9lSYgG"}},{"cell_type":"code","source":["#from devtools import *\n","#from sp import *\n","#from raster import *\n","#from keras import *\n","#from tensorflow import *\n","#from spatstat import *\n","#from magick import *\n","#from imager import *\n","#from jpeg import *\n","#from tibbl import *\n","#from ggplot2 import *\n","#from tidyr import *\n","#from dplyr import *\n","#from caret import *\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow import keras\n","from tensorflow.keras.datasets import mnist \n","import keras_preprocessing\n","from keras_preprocessing import image\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","\n","import cv2\n","#from google.colab.patches import cv2_imshow\n","import pandas as pd\n","import numpy as np \n","import matplotlib\n","import matplotlib.pyplot as plt\n","import os, math\n","from urllib.request import urlopen,urlretrieve\n","from PIL import Image, ImageDraw, ImageFont\n","from tqdm import tqdm_notebook\n","from glob import glob\n","from sklearn.datasets import load_files   \n","from sklearn.utils import shuffle\n","from keras import applications\n","from keras.utils import np_utils\n","from keras.models import Sequential,Model,load_model\n","from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, BatchNormalization, GaussianNoise\n","from keras.callbacks import TensorBoard,ReduceLROnPlateau,ModelCheckpoint\n","from tensorflow.keras.optimizers import SGD, Adam\n","from keras.preprocessing.image import ImageDataGenerator \n","from keras.applications.imagenet_utils import decode_predictions\n","from skimage.io import imread\n","from skimage.transform import resize\n","from sklearn import svm, datasets\n","from skimage.util import view_as_windows\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils.multiclass import unique_labels\n"],"metadata":{"id":"54VqG7MxSiuL","executionInfo":{"status":"ok","timestamp":1662002028336,"user_tz":-420,"elapsed":5146,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["1-PATH DEFINITION"],"metadata":{"id":"7QGT9wWJS7Dn"}},{"cell_type":"code","source":["#Extract Zip\n","from zipfile import ZipFile\n","file_name = \"/content/drive/MyDrive/BKK/DATASET/dataset_nature.zip\"\n","with ZipFile (file_name,'r') as zip:\n","  zip.extractall()\n","\n","#Define the path for the dataset folder\n","\n","path = '/content' #file directory\n","path_dataset = (path + '/dataset_nature') #dataset directory"],"metadata":{"id":"0bFNg4TnS9t8","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1662002081537,"user_tz":-420,"elapsed":383,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"1b0348dc-d41e-4017-db96-124053948e8e"},"execution_count":4,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a45fe84da7da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/BKK/DATASET/dataset_nature.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mZipFile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1238\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1241\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/BKK/DATASET/dataset_nature.zip'"]}]},{"cell_type":"code","source":["#Define the path for the dataset sub-folders\n","\n","path_train = (path_dataset + '/training')\n","path_val = (path_dataset + '/validation')\n","path_test = (path_dataset + '/test')\n","\n","print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"3rqCx7NfTE2u","executionInfo":{"status":"error","timestamp":1662002140122,"user_tz":-420,"elapsed":452,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"c487b676-5235-4806-e1fa-0e0ced43b3fc"},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e399ac131f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Define the path for the dataset sub-folders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpath_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpath_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'path_dataset' is not defined"]}]},{"cell_type":"markdown","source":["2-PRE DEFINITION OF THE MODEL"],"metadata":{"id":"KJE7ZhnkTJLV"}},{"cell_type":"code","source":["#Keras sorts the labels (names of folders in the train directory) by alphabetical order.\n","\n","num_classes=2\n","class_names = ('a_nature', 'others')"],"metadata":{"id":"CwbBCWwPTQs4","executionInfo":{"status":"ok","timestamp":1662002143951,"user_tz":-420,"elapsed":1308,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# parameters layers\n","filters_1= 16\n","filters_2= 16\n","filters_3= 16\n","filters_4= 16\n","filters_dense = 16\n","size = 64\n","kernel_size = (3, 3)\n","pool_size = 2\n","rate = 0.4\n","#mean = 0\n","batch_size = 32 #batch_size to train\n","nb_epoch  = 10\n","learning_rate  = 0.0001\n","img_rows, img_cols = 64, 64\n","\n","print('Done')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYSncb1RUPY9","executionInfo":{"status":"ok","timestamp":1662002155978,"user_tz":-420,"elapsed":467,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"6226f81f-186a-4137-b1de-28be000536af"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Done\n"]}]},{"cell_type":"markdown","source":["3-DATA AUGMENTATION"],"metadata":{"id":"qNXiTUWPUgzN"}},{"cell_type":"code","source":["# Artificially improving the dataset with image data generator (rotating images to artificially generate more dataset)\n","\n","train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n","  rescale = 1/255,\n","  rotation_range = 40,\n","  width_shift_range = 0.2,\n","  height_shift_range = 0.2,\n","  shear_range = 0.2,\n","  zoom_range = 0.2,\n","  horizontal_flip = True,\n","  fill_mode = \"nearest\"\n","  )\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)  \n","\n","train_generator = train_datagen.flow_from_directory(\n","    directory= path_train,              #target directory \n","    target_size=(size, size),           #resizes all images\n","    color_mode=\"rgb\",\n","    batch_size=batch_size,\n","    class_mode=\"categorical\",\n","    shuffle=True)\n","  \n","validation_generator = test_datagen.flow_from_directory(\n","    directory= path_val,\n","    target_size=(size,size),\n","    color_mode=\"rgb\",\n","    batch_size=batch_size,\n","    class_mode=\"categorical\",\n","    shuffle=True,\n","    seed=42) #Batch size is done by default by keras\n","\n","test_generator = test_datagen.flow_from_directory(\n","    directory= path_test,\n","    target_size=(size, size),\n","    color_mode=\"rgb\",\n","    batch_size=1,\n","    class_mode=\"categorical\",\n","    shuffle=True)\n","\n","labels = train_generator.class_indices\n","#label_nb = np.size(list((labels.values())))\n","label_nb = train_generator.num_classes # is the same\n","label_names = list((labels.keys()))\n","num_classes = label_nb"],"metadata":{"id":"XVMeKPZUbzqg","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1662002161481,"user_tz":-420,"elapsed":38,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"149b7aeb-b3b0-483b-dd79-076ef9708982"},"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-caee2c9917d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m train_generator = train_datagen.flow_from_directory(\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpath_train\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m#target directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0;31m#resizes all images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mcolor_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'path_train' is not defined"]}]},{"cell_type":"markdown","source":["4-DEFINING THE MODEL (NUMBER OF LAYERS AND PARAMETERS)"],"metadata":{"id":"Zc6J6ZbFcX6L"}},{"cell_type":"code","source":["##Creation of Layers\n","model = Sequential()                #Model creation\n","\n","##LAYER 1##\n","layer1 = Conv2D(filters = filters_1, kernel_size= kernel_size, input_shape = (size, size, 3), activation = 'relu')\n","model.add(layer1)\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size= pool_size ))\n","model.add(Dropout(rate))\n","\n","##LAYER 2##\n","layer2 = Conv2D(filters = filters_2, kernel_size = kernel_size, activation = 'relu')\n","model.add(layer2)\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size= pool_size ))\n","model.add(Dropout(rate))\n","\n","##LAYER 3##\n","layer3 = Conv2D(filters = filters_3, kernel_size = kernel_size, activation = 'relu')\n","model.add(layer3)\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size= pool_size ))\n","model.add(Dropout(rate))\n","\n","##LAYER 4##\n","layer4 = Conv2D(filters = filters_4, kernel_size = kernel_size, activation = 'relu')\n","model.add(layer4)\n","model.add(BatchNormalization())\n","model.add(MaxPooling2D(pool_size= pool_size ))\n","model.add(Dropout(rate))\n","model.add(Flatten())\n","\n","##LAYER 5##\n","layer5 = Dense(filters_dense,activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))\n","model.add(layer5)\n","model.add(BatchNormalization())\n","model.add(Dropout(rate))\n","\n","##LAYER 6##\n","layer6 = Dense(filters_dense,activation = \"relu\", kernel_regularizer = tf.keras.regularizers.l1(0.01))\n","model.add(layer6)\n","model.add(BatchNormalization())\n","model.add(Dropout(rate))\n","\n","##LAYER 7##\n","layer7=Dense(units = num_classes, activation = 'softmax')     # units: number of nodes in the layer\n","model.add(layer7)\n","\n","model.summary()"],"metadata":{"id":"xXB85yezceLS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662002246893,"user_tz":-420,"elapsed":1254,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"cfaa5eb6-a37e-42d2-b3f4-a545aabefe27"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 16)        448       \n","                                                                 \n"," batch_normalization (BatchN  (None, 62, 62, 16)       64        \n"," ormalization)                                                   \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 31, 31, 16)       0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 31, 31, 16)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 16)        2320      \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 29, 29, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 14, 14, 16)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 14, 14, 16)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 12, 12, 16)        2320      \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 12, 12, 16)       64        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 6, 6, 16)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_2 (Dropout)         (None, 6, 6, 16)          0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 4, 4, 16)          2320      \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 4, 4, 16)         64        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 2, 2, 16)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 2, 2, 16)          0         \n","                                                                 \n"," flatten (Flatten)           (None, 64)                0         \n","                                                                 \n"," dense (Dense)               (None, 16)                1040      \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 16)               64        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_4 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 16)                272       \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 16)               64        \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_5 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 2)                 34        \n","                                                                 \n","=================================================================\n","Total params: 9,138\n","Trainable params: 8,946\n","Non-trainable params: 192\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["5-MODEL TRAINING"],"metadata":{"id":"SjBkX74IcjRo"}},{"cell_type":"code","source":["opt = Adam(learning_rate=learning_rate)\n","model.compile(optimizer = opt, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","#Early stopping the model training when the model is stable enough\n","from keras.callbacks import EarlyStopping\n","\n","#Plot\n","speed = 1\n","STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size//speed\n","STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size//speed\n","STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n","\n","#Callback\n","callback = EarlyStopping(monitor = 'val_loss',\n","                          min_delta = 0,\n","                          patience = 30, #Number of epochs after minimum\n","                          verbose = 1,\n","                          restore_best_weights = True)\n","\n","#Model training\n","h = model.fit(train_generator,\n","                    steps_per_epoch=20,\n","                    validation_data=validation_generator,\n","                    validation_steps=STEP_SIZE_VALID,\n","                    epochs=nb_epoch, \n","                    callbacks = [callback])"],"metadata":{"id":"FhFerec8cl6P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["6-CREATION OF A RESULTS FOLDER"],"metadata":{"id":"dBXUXGDPcpam"}},{"cell_type":"code","source":["import time\n","#end = time.time()\n","#print(end - start)\n","#print(learning_rate)\n","class_names = ['nature','others']\n","path_base = '/content/drive/MyDrive/BKK/IA'\n","path_result = path_base + '/RESULTS'\n","list = os.listdir(path_result)\n","nb_test = len(list) #We take a look at the number of tests already existing in the destination folder\n","test_name = 'GN'+ str(nb_test+1) + '_' + class_names[0] + '_' + class_names[1] + '_filters_' + str(filters_1) + '_batch-size_' + str(batch_size) #We assign a new number to the current test\n","index = 1 #index for the part 9 MAPPING\n","\n","os.mkdir(path_result+'/'+test_name)\n","path_test = path_result+'/'+test_name\n","model.save_weights(path_test + '/'  + test_name + '_weights.h5')\n","model.save(path_test + '/'  + test_name + '_model.h5')\n","print('Done')"],"metadata":{"id":"Kx4nflPhcsd9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1662002259249,"user_tz":-420,"elapsed":430,"user":{"displayName":"Valentí Riba Revert","userId":"03387634847332745795"}},"outputId":"ad077d21-8923-4d51-a7bc-3204c30748e6"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["Done\n"]}]},{"cell_type":"markdown","source":["7-EVALUATION OF THE TRAINING LOSS AND ACCURACY ON THE DATASET"],"metadata":{"id":"5dF2l8KScvIZ"}},{"cell_type":"code","source":["N = len(h.history['accuracy']) #N is the number of epochs considering the early stopping\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0,N), h.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), h.history[\"val_loss\"], label=\"val_loss\")\n","plt.plot(np.arange(0, N), h.history[\"accuracy\"], label=\"train_accuracy\")\n","plt.plot(np.arange(0, N), h.history[\"val_accuracy\"], label=\"val_accuracy\")\n","plt.title(\"Training Loss and Accuracy on Dataset\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss/Accuracy\")\n","plt.legend(loc=\"center right\")\n","# plt.savefig(path_test + '/'  + test_name + '_1.png')\n","\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), h.history[\"loss\"], label=\"train_loss\")\n","plt.plot(np.arange(0, N), h.history[\"val_loss\"], label=\"val_loss\")\n","plt.title(\"Training Loss on Dataset\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend(loc=\"upper right\")\n","# plt.savefig(path_test + '/'  + test_name +  '_2.png')\n","\n","plt.style.use(\"ggplot\")\n","plt.figure()\n","plt.plot(np.arange(0, N), h.history[\"accuracy\"], label=\"train_accuracy\")\n","plt.plot(np.arange(0, N), h.history[\"val_accuracy\"], label=\"val_accuracy\")\n","plt.title(\"Training Accuracy on Dataset\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend(loc=\"lower right\")\n","# plt.savefig(path_test + '/'  + test_name + '_3.png')"],"metadata":{"id":"46XfCX9sc1hU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["8-CONFUSION MATRIX & CLASSIFICATION REPORT"],"metadata":{"id":"ZMi-zOO0c8eR"}},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","\n","#Evaluate\n","test_generator.reset()\n","scores = model.evaluate(test_generator, steps= test_generator.samples, verbose=1)\n","print(\"Test Accuracy = %.2f%%\" % (scores[1]*100))\n","\n","#Predict\n","y_pred = model.predict(test_generator, steps = test_generator.samples, verbose=1)\n","y_pred = np.argmax(y_pred, axis=-1)\n","y_true = test_generator.classes[test_generator.index_array]\n","\n","# accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(y_true, y_pred)\n","#print('Accuracy: %f' % accuracy)\n","# precision tp / (tp + fp)\n","precision = precision_score(y_true, y_pred, average='macro')\n","#print('Precision: %f' % precision)\n","# recall: tp / (tp + fn)\n","recall = recall_score(y_true, y_pred, average='macro')\n","#print('Recall: %f' % recall)\n","# f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(y_true, y_pred,average='macro')\n","#print('F1 score: %f' % f1)\n","\n","data_label = ['Accuracy: ', 'Precision:' ,'Recall:', 'F1 Score:']\n","data = [ accuracy, precision,recall,f1]\n","param_label = ['Learning Rate:', 'Batch Size:','Nombre d époches:', 'Filter_1:','Filter_2:','Filter_3:','Filter_4:','Pool Size:', 'Dropout Rate:']\n","param = [learning_rate,batch_size,N,filters_1,filters_2,filters_3,filters_4,pool_size,rate]\n","score = pd.DataFrame({'jhv':data_label,'jhvh':data})\n","param = pd.DataFrame({'jhvf':param_label,'jhcv':param})\n","df_score=pd.DataFrame(score)\n","df_param=pd.DataFrame(param)\n","df_tot = pd.concat([df_score, df_param], ignore_index=True, axis=1)\n","# df_tot.to_csv (path_test +'/valeurs_' + test_name + '.csv', index = False, header=True)   \n","# mettre les valeurs dans deux colonnes différentes \n","print (df_tot)\n","\n","# Confusion Matrix and Classification Repport\n","target_names=['Canals','Rest']\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","print('Confusion Matrix')\n","cm = confusion_matrix(y_true, y_pred)\n","print('Classification Report')\n","\n","#This function prints and plots the confusion matrix.\n","#Normalization can be applied by setting `normalize=True`.\n","def plot_confusion_matrix(y_true, y_pred, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","    print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","    \n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return fig\n","\n","np.set_printoptions(precision=1)\n","\n","# Plot non-normalized confusion matrix\n","fig = plot_confusion_matrix(test_generator.classes[test_generator.index_array],y_pred, classes = target_names,\n","                      normalize = True, title='Normalized Confusion matrix')\n","# fig.savefig(path_test + '/' + test_name + '_CM.png')   "],"metadata":{"id":"J_mXG198dB1j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["9-PREDICTION AND MAPPING"],"metadata":{"id":"N6uKZAL1dHLF"}},{"cell_type":"code","source":["window_width, window_heigth  = 64, 64 \n","overlap = 16 #must be a factor for window_width, window_heigth\n","\n","model = load_model('/content/drive/MyDrive/BKK/IA/RESULTS/IA_NATURE/GN3_Others_Towers_filters_16_batch-size_256_model.h5')\n","\n","# Creating a loop to apply the prediction on several images\n","for l in range(1,8):\n","  print('Treating image number',l,'...')\n","\n","  #Opening the image to apply the prediction\n","  img_tocrop=Image.open('/content/drive/MyDrive/BKK/MAP' + str(l) + '.jpg')\n","  #img_tocrop = Image.open(path_base +  '/Detection/cropped_image_' + str(l) +'.jpg')\n","\n","\n","  # cropping the image to predict into a multiple of the window dimensions\n","  img_w, img_h  = img_tocrop.size\n","  wrest = img_w%window_width #%=remainder, // = integer\n","  hrest = img_h%window_heigth\n","  obj_image = img_tocrop.crop((0, 0, img_w-wrest, img_h-hrest)) \n","\n","  # Cut the image in the corresponging proportional piezes\n","  img_width, img_height  = obj_image.size\n","  nb_img_width = int(img_width//overlap)\n","  nb_img_heigth = int(img_height//overlap)\n","  nb_total = nb_img_width*nb_img_heigth\n","  obj_subimages = np.zeros((nb_total, window_width*window_heigth*3), dtype = int) \n","  obj_subimages = obj_subimages.reshape(nb_total, window_width, window_heigth, 3)\n","\n","  # Creation of matrixes to make prediction of the subimages\n","  k=0\n","  obj_subimages_pred = np.zeros((nb_total, 1), dtype = int)\n","  obj_subimages_accu = np.zeros((nb_total, 1), dtype = float)\n","\n","  # Creation of matrixes to collect the prediction values and accuracies\n","  mapping = np.zeros((img_height, img_width), dtype = int)\n","  accura = np.zeros((img_height, img_width), dtype = int)     \n","\n","  for i in range(nb_img_heigth):\n","      for j in range(nb_img_width):\n","          box = (j*overlap, i*overlap, window_width + j*overlap, window_heigth + i*overlap)\n","          cropped = obj_image.crop(box)\n","          cropped = np.array(cropped)\n","          obj_subimages[k,:, :, :] = cropped\n","          \n","          #Make prediction of the subimages\n","          x = obj_subimages[k,:,:,:]\n","          x = np.expand_dims(x, 0)\n","          y = model.predict(x/255)\n","          yy = np.argmax(y)\n","          yyy = round(np.max(y)*100, 2)\n","          yy_matrix =   yy*np.ones((overlap, overlap), dtype = int)\n","          yyy_matrix = yyy*np.ones((overlap, overlap), dtype = int)\n","\n","          #matrixes to collect the prediction values and accuracies\n","          mapping[i*overlap:i*overlap+overlap,j*overlap:j*overlap+overlap] = yy_matrix   #y1:y2+1, x1:x2+1\n","          accura[i *overlap:i*overlap+overlap,j*overlap:j*overlap+overlap]  = yyy_matrix\n","          k += 1\n","  print('Image number',l,'treated.')\n","\n","  plt.figure()\n","  colors = 'green yellow'.split() #canals city forest road\n","  cmap = matplotlib.colors.ListedColormap(colors, name='colors', N=None)\n","  plt.imshow(mapping, cmap=cmap, vmin=0, vmax= (num_classes - 1)) #vmin and vmax give the number of colors to use\n","  plt.title('Prediction map')\n","  plt.savefig(path_test + '/image_' + str(l) + '_overlap_' + str(overlap) + '_mapping.png')\n","  pd.DataFrame(mapping).to_csv(path_test + '/matrice_classes_image_' + str(l) + '_overlap_' + str(overlap) + '.csv', index = False, header=False)\n","  pd.DataFrame(accura).to_csv(path_test  + '/matrice_accu_image_' + str(l) + '_overlap_' + str(overlap) + '.csv', index = False, header=False)\n"],"metadata":{"id":"CVpJg8DMdJ6Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de8959d3-a020-4415-b7bc-13349b7adb83"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Treating image number 1 ...\n"]}]}]}